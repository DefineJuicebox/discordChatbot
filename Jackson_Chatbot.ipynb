{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPV8HXb8jpsl",
        "outputId": "f5c71d99-6441-4dd3-e2f0-005ed8a13232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ],
      "id": "IPV8HXb8jpsl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y0M2Z4ZjohE",
        "outputId": "328d291f-56e5-451e-a26c-05b633084354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting discord\n",
            "  Downloading discord-2.2.2-py3-none-any.whl (1.1 kB)\n",
            "Collecting discord.py>=2.2.2\n",
            "  Downloading discord.py-2.2.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4,>=3.7.4\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.2.2->discord) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.2.2->discord) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.9/dist-packages (from yarl<2.0,>=1.0->aiohttp<4,>=3.7.4->discord.py>=2.2.2->discord) (3.4)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, discord.py, discord\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 discord-2.2.2 discord.py-2.2.2 frozenlist-1.3.3 multidict-6.0.4 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "pip install discord"
      ],
      "id": "2Y0M2Z4ZjohE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8zGD2SYJ-zJ",
        "outputId": "42089ee6-6d28-4578-de34-d0917029baf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-fuzzy\n",
            "  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from scikit-fuzzy) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from scikit-fuzzy) (1.10.1)\n",
            "Requirement already satisfied: networkx>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from scikit-fuzzy) (3.1)\n",
            "Building wheels for collected packages: scikit-fuzzy\n",
            "  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894086 sha256=dcd231821cfbd042328d7c028dc880501e559ef9b866d1ab189bf2511c13b5a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/2c/a1/a90a7d7dd8448ec029f298a61f3490275e99b17aa348be675c\n",
            "Successfully built scikit-fuzzy\n",
            "Installing collected packages: scikit-fuzzy\n",
            "Successfully installed scikit-fuzzy-0.4.2\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-fuzzy"
      ],
      "id": "h8zGD2SYJ-zJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61cb011b",
        "outputId": "e63b97e1-676b-48ec-9f6a-856c691a6def"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import discord\n",
        "import os\n",
        "import nest_asyncio\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import skfuzzy as fuzz\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from nltk.tokenize import word_tokenize\n",
        "from skfuzzy import control as ctrl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "nltk.download('punkt')\n",
        "nest_asyncio.apply()"
      ],
      "id": "61cb011b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_3a1ZfrTn7W",
        "outputId": "5ef8aac0-c325-4e74-f8d7-8b2985fcef62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.5304 - accuracy: 0.7332\n",
            "Epoch 2/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.7518\n",
            "Epoch 3/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7637\n",
            "Epoch 4/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.7762\n",
            "Epoch 5/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7795\n",
            "Epoch 6/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7814\n",
            "Epoch 7/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7842\n",
            "Epoch 8/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7814\n",
            "Epoch 9/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.7795\n",
            "Epoch 10/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.7801\n",
            "Epoch 11/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7781\n",
            "Epoch 12/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7845\n",
            "Epoch 13/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7778\n",
            "Epoch 14/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7814\n",
            "Epoch 15/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.7778\n",
            "Epoch 16/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7831\n",
            "Epoch 17/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7825\n",
            "Epoch 18/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7781\n",
            "Epoch 19/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7806\n",
            "Epoch 20/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7812\n",
            "Epoch 21/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4508 - accuracy: 0.7801\n",
            "Epoch 22/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.7831\n",
            "Epoch 23/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.7823\n",
            "Epoch 24/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7856\n",
            "Epoch 25/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7837\n",
            "Epoch 26/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.7784\n",
            "Epoch 27/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.7814\n",
            "Epoch 28/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.7812\n",
            "Epoch 29/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7801\n",
            "Epoch 30/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4467 - accuracy: 0.7812\n",
            "Epoch 31/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.7859\n",
            "Epoch 32/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.7839\n",
            "Epoch 33/50\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.7837\n",
            "Epoch 34/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7848\n",
            "Epoch 35/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7820\n",
            "Epoch 36/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7834\n",
            "Epoch 37/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7842\n",
            "Epoch 38/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7801\n",
            "Epoch 39/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7806\n",
            "Epoch 40/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7823\n",
            "Epoch 41/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7864\n",
            "Epoch 42/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7859\n",
            "Epoch 43/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7828\n",
            "Epoch 44/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7828\n",
            "Epoch 45/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7848\n",
            "Epoch 46/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7823\n",
            "Epoch 47/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7892\n",
            "Epoch 48/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.7820\n",
            "Epoch 49/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.7853\n",
            "Epoch 50/50\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7886\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff252ea2f70>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load data CSV\n",
        "data = pd.read_csv('HatchData.csv')\n",
        "\n",
        "#Separate the input and output data\n",
        "input_data = data[['Bulb_Data', 'Fan_Data', 'Humidity_Data', 'Temperature_Data']]\n",
        "output_data = data['Valve_Data']\n",
        "\n",
        "#Preprocess the input data using standardization\n",
        "scaler = StandardScaler()\n",
        "input_data = scaler.fit_transform(input_data)\n",
        "\n",
        "#Create Keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=4, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Train model\n",
        "model.fit(input_data, output_data, epochs=50, batch_size=32)\n"
      ],
      "id": "f_3a1ZfrTn7W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC5NNmT6kI64"
      },
      "source": [],
      "id": "cC5NNmT6kI64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2d89969",
        "outputId": "f3477317-e18d-4188-d95e-4f03d3e0c5e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:discord.client:PyNaCl is not installed, voice will NOT be supported\n"
          ]
        }
      ],
      "source": [
        "#Discord Bot Token\n",
        "TOKEN = 'MTA5MDkyMjc3NjIwODA4NTA2NA.GTx0PW.IwqO5NV1oO-HnUA1uNGX1XQX0gPOm7dNd__T9o'\n",
        "client = discord.Client(intents=discord.Intents.all())"
      ],
      "id": "d2d89969"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFL1oOqGXOYR"
      },
      "outputs": [],
      "source": [
        "#TensorFlow Prediction\n",
        "  #Creating a function to call it\n",
        "def predictTensorflow(bulb,fan,humidity,temperature):\n",
        "  inputData = [[bulb,fan,humidity,temperature]]\n",
        "  inputData = scaler.transform(inputData)\n",
        "  prediction = model.predict(inputData)\n",
        "  return prediction[0][0]"
      ],
      "id": "pFL1oOqGXOYR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dccaca21"
      },
      "outputs": [],
      "source": [
        " #Fuzzy Logic\n",
        "    #Creating a function to call it later\n",
        "def generatePrediction(inputHumidity,inputTemperature):\n",
        "\n",
        "      #Create antecedents and consequents\n",
        "    temp = ctrl.Antecedent(np.arange(0, 101, 1), 'temperature')\n",
        "    humid = ctrl.Antecedent(np.arange(0, 101, 1), 'humidity')\n",
        "    incub = ctrl.Consequent(np.arange(0, 101, 1), 'incubation')\n",
        "\n",
        "      #Define membership functions for temperature\n",
        "    temp['cold'] = fuzz.trapmf(temp.universe, [0, 0, 20, 30])\n",
        "    temp['cool'] = fuzz.trimf(temp.universe, [20, 30, 40])\n",
        "    temp['moderate'] = fuzz.trimf(temp.universe, [30, 40, 50])\n",
        "    temp['warm'] = fuzz.trimf(temp.universe, [40, 50, 60])\n",
        "    temp['hot'] = fuzz.trapmf(temp.universe, [50, 60, 100, 100])\n",
        "\n",
        "      #Define membership functions for humidity\n",
        "    humid['dry'] = fuzz.trapmf(humid.universe, [0, 0, 20, 40])\n",
        "    humid['low'] = fuzz.trimf(humid.universe, [20, 40, 60])\n",
        "    humid['moderate'] = fuzz.trimf(humid.universe, [40, 60, 80])\n",
        "    humid['high'] = fuzz.trapmf(humid.universe, [60, 80, 100, 100])\n",
        "\n",
        "      #Define membership functions for success\n",
        "    incub['fail'] = fuzz.trapmf(incub.universe, [0, 0, 40, 60])\n",
        "    incub['success'] = fuzz.trapmf(incub.universe, [40, 60, 100, 100])\n",
        "\n",
        "      #Define the rules\n",
        "    rule1 = ctrl.Rule(temp['cold'] & humid['dry'], incub['fail'])\n",
        "    rule2 = ctrl.Rule(temp['cool'] & humid['dry'], incub['fail'])\n",
        "    rule3 = ctrl.Rule(temp['moderate'] & humid['dry'], incub['fail'])\n",
        "    rule4 = ctrl.Rule(temp['warm'] & humid['dry'], incub['fail'])\n",
        "    rule5 = ctrl.Rule(temp['hot'] & humid['dry'], incub['fail'])\n",
        "\n",
        "    rule6 = ctrl.Rule(temp['cold'] & humid['low'], incub['fail'])\n",
        "    rule7 = ctrl.Rule(temp['cool'] & humid['low'], incub['fail'])\n",
        "    rule8 = ctrl.Rule(temp['moderate'] & humid['low'], incub['fail'])\n",
        "    rule9 = ctrl.Rule(temp['warm'] & humid['low'], incub['success'])\n",
        "    rule10 = ctrl.Rule(temp['hot'] & humid['low'], incub['fail'])\n",
        "\n",
        "    rule11 = ctrl.Rule(temp['cold'] & humid['moderate'], incub['fail'])\n",
        "    rule12 = ctrl.Rule(temp['cool'] & humid['moderate'], incub['fail'])\n",
        "    rule13 = ctrl.Rule(temp['moderate'] & humid['moderate'], incub['success'])\n",
        "    rule14 = ctrl.Rule(temp['warm'] & humid['moderate'], incub['success'])\n",
        "    rule15 = ctrl.Rule(temp['hot'] & humid['moderate'], incub['fail'])\n",
        "\n",
        "    rule16 = ctrl.Rule(temp['cold'] & humid['high'], incub['fail'])\n",
        "    rule17 = ctrl.Rule(temp['cool'] & humid['high'], incub['success'])\n",
        "    rule18 = ctrl.Rule(temp['moderate'] & humid['high'], incub['success'])\n",
        "    rule19 = ctrl.Rule(temp['warm'] & humid['high'], incub['success'])\n",
        "    rule20 = ctrl.Rule(temp['hot'] & humid['high'],incub['fail'])\n",
        "\n",
        "      #Define the Control System with the rules\n",
        "    egg_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5,rule6,rule7,rule8,rule9,rule10,rule11,rule12,rule13,rule14,rule15,rule16,rule17,rule18,rule19,rule20])\n",
        "\n",
        "    eggHatch = ctrl.ControlSystemSimulation(egg_ctrl)\n",
        "    eggHatch.input['temperature'] = inputTemperature\n",
        "    eggHatch.input['humidity'] = inputHumidity\n",
        "\n",
        "      #Compute the predictive measurement with the temperaturea and humidity provided\n",
        "    eggHatch.compute()\n",
        "    x = eggHatch.output['incubation']\n",
        "    return x\n",
        "\n",
        "    temperature.view()\n",
        "    humidity.view()\n",
        "    incubation.view()\n"
      ],
      "id": "dccaca21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "05c5eef1",
        "scrolled": true,
        "outputId": "86a40722-6266-4895-974c-04955c67dc2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[30;1m2023-04-20 06:34:18\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "INFO:discord.client:logging in using static token\n",
            "\u001b[30;1m2023-04-20 06:34:18\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: b86d4c7bb3ad0c35947df8c2041f9401).\n",
            "INFO:discord.gateway:Shard ID None has connected to Gateway (Session ID: b86d4c7bb3ad0c35947df8c2041f9401).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as Jackson Egg Incubator#3673\n",
            "Jackson Bot is Ready!\n",
            "Received message: oi\n",
            "Received message: predict\n",
            "Received message: fuzzy logic\n",
            "Received message: can you predict egg\n",
            "1/1 [==============================] - 0s 115ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received message: tensorflow\n",
            "Received message: hi\n",
            "Received message: hihihihi\n",
            "Received message: hihi\n",
            "Received message: halo\n",
            "Received message: bye\n",
            "Received message: temperature\n",
            "Received message: day\n",
            "Received message: valva\n",
            "Received message: valve\n",
            "Received message: May I know the temperature of the egg\n",
            "Received message: May I know the humidityof the egg\n",
            "Received message: May I know the humidity of the egg\n",
            "Received message: May I know the temperature of the egg\n",
            "Received message: May I know the fan of the egg\n",
            "Received message: May I know the bulb status of the egg\n",
            "Received message: May I know the bulb on time of the egg\n",
            "Received message: May I know the day of the egg\n",
            "Received message: May I know the valve of the egg\n",
            "Received message: what kind of egg do you know\n",
            "Received message: chicken egg\n",
            "Received message: duck egg\n",
            "Received message: goose egg\n",
            "Received message: Can you predict the incubation rate\n",
            "Received message: Fuzzy logic\n",
            "Received message: Predict the egg\n",
            "1/1 [==============================] - 0s 40ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received message: Tensorflow\n",
            "Received message: ok\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[30;1m2023-04-20 08:20:37\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session b86d4c7bb3ad0c35947df8c2041f9401.\n",
            "INFO:discord.gateway:Shard ID None has successfully RESUMED session b86d4c7bb3ad0c35947df8c2041f9401.\n"
          ]
        }
      ],
      "source": [
        "#Scrape json file\n",
        "url = \"https://eggincubator-825e1-default-rtdb.firebaseio.com/.json\"\n",
        "respond = requests.get(url)\n",
        "data = json.loads(respond.text)\n",
        "\n",
        "#Scrape specific data\n",
        "bulbTime = data['Bulb on times:']\n",
        "bulbStat = data['BulbStatus']\n",
        "incubatorDay = data['Day']\n",
        "incubatorFan = data['FanStatus']\n",
        "incubatorHumidity = data['Humidity']\n",
        "incubatorTemperature = data['Temperature']\n",
        "valveStat = data['ValveStatus']\n",
        "\n",
        "#Function to preprocess user input for Bag of Words NLP\n",
        "def preprocess_input(input_text):\n",
        "    tokens = nltk.word_tokenize(input_text.lower())\n",
        "    return tokens\n",
        "\n",
        "def generateAnswer(input_incubator):\n",
        "    tokens = preprocess_input(input_incubator)\n",
        "    for token in tokens:\n",
        "      if token in incubator_responses:\n",
        "        return random.choice(incubator_responses[token])\n",
        "    return \"Sorry, I don't understand.\"\n",
        "\n",
        "#Bags of Words for different responses\n",
        "incubator_responses = {\n",
        "  'day': ['The Day is', 'Day is','Let it be known that the day is '],\n",
        "  'temperature': ['The Temperatre is', 'Temperature is'],\n",
        "  'humidity': ['The Humidity is', 'Humidity in the incubator is','Current Humidity is'],\n",
        "  'fan': ['The incubator fan is ', 'Fan is', 'Currently the fan is '],\n",
        "  'valve': ['The valve is now ', 'Valve is ','Currently the valve is ']\n",
        "}\n",
        "\n",
        "#Identify different types of egg\n",
        "egg_types = {  #Dictionary of egg types and their characteristics\n",
        "  'chicken': {\n",
        "    'incubation_time': 21,\n",
        "    'ideal_temperature': 37.5,\n",
        "    'ideal_humidity': (50, 55),\n",
        "    'turning_of_eggs': 3,\n",
        "  },\n",
        "  'duck': {\n",
        "    'incubation_time': 28,\n",
        "    'ideal_temperature': 37.5,\n",
        "    'ideal_humidity': (60, 65),\n",
        "    'turning_of_eggs': 4,\n",
        "  },\n",
        "  'goose': {\n",
        "    'incubation_time': 28,\n",
        "    'ideal_temperature': 37.5,\n",
        "    'ideal_humidity': (70, 75),\n",
        "    'turning_of_eggs': 5,\n",
        "  }\n",
        "}\n",
        "\n",
        "class JacksonBot(discord.Client):\n",
        "\n",
        "    async def on_ready(self):\n",
        "        print(f'Logged in as {self.user}')\n",
        "        print('Jackson Bot is Ready!')\n",
        "\n",
        "      #Chatbot will greet once ready\n",
        "        channel = self.get_channel(1083616177185620054)\n",
        "        await channel.send(\"Hi, my name is Jackson.\")\n",
        "        await channel.send(\"How can I help you?\")\n",
        "        await channel.send(\"Tip 1: Ask me the real-time egg incubating conditions like temperature, humidity, fan, bulb status, bulb on time, day and valve.\")\n",
        "        await channel.send(\"Tip 2: Ask the condition for hatching a specific egg.\")\n",
        "        await channel.send(\"Tip 3: Ask the prediction of incubating the egg by providing the humidity and temperature. (Use 'predict' as keyword)\")\n",
        "\n",
        "    async def on_message(self,message):\n",
        "        try:\n",
        "               #Avoid the bot replying to ownself\n",
        "            if message.author.id == self.user.id:\n",
        "                return\n",
        "\n",
        "              #Temperature Status Here\n",
        "            if \"temperature\" in message.content.lower():\n",
        "                incubatorInput = message.content.lower();\n",
        "                incubatorRespond = generateAnswer(incubatorInput)\n",
        "                await message.channel.send(f'{incubatorRespond} {incubatorTemperature} degrees Celsius.')\n",
        "\n",
        "              #Humidity Status Here\n",
        "            elif \"humidity\" in message.content.lower():\n",
        "                incubatorInput = message.content.lower();\n",
        "                incubatorRespond = generateAnswer(incubatorInput)\n",
        "                await message.channel.send(f'{incubatorRespond} {incubatorHumidity}.')\n",
        "\n",
        "              #Fan Status Here\n",
        "            elif \"fan\" in message.content.lower():\n",
        "                if incubatorFan == 0:\n",
        "                  fanStat = \"off\"\n",
        "                else:\n",
        "                  fanStat = \"on\"\n",
        "                incubatorInput = message.content.lower();\n",
        "                incubatorRespond = generateAnswer(incubatorInput)\n",
        "                await message.channel.send(f'{incubatorRespond} {fanStat}.')\n",
        "\n",
        "              #Bulb Status Here\n",
        "            elif \"bulb status\" in message.content.lower():\n",
        "                if bulbStat == 0:\n",
        "                  bulb_stat = \"off\"\n",
        "                else:\n",
        "                  bulb_stat = \"on\"\n",
        "                messageBulbStat = ['The current bulb status is','The bulb status is','The bulb is']\n",
        "                res = messageBulbStat[random.randint(0, 2)]\n",
        "                await message.channel.send(f'{res.format(message)} {bulb_stat}.')\n",
        "\n",
        "              #Bulb on Time Here\n",
        "            elif \"bulb on time\" in message.content.lower():\n",
        "                messageBulbTime = ['The current bulb time is','The bulb time is','The bulb has been on for']\n",
        "                res = messageBulbTime[random.randint(0, 2)]\n",
        "                await message.channel.send(f'{res.format(message)} {bulbTime} minutes.')\n",
        "\n",
        "              #Incubator Day Here\n",
        "            elif \"day\" in message.content.lower():\n",
        "                incubatorInput = message.content.lower();\n",
        "                incubatorRespond = generateAnswer(incubatorInput)\n",
        "                await message.channel.send(f'{incubatorRespond} {incubatorDay}.')\n",
        "\n",
        "              #Valve Status Here\n",
        "            elif \"valve\" in message.content.lower():\n",
        "                if valveStat == 0:\n",
        "                  valve_stat = \"off\"\n",
        "                else:\n",
        "                  valve_stat = \"on\"\n",
        "                incubatorInput = message.content.lower();\n",
        "                incubatorRespond = generateAnswer(incubatorInput)\n",
        "                await message.channel.send(f'{incubatorRespond}{valve_stat}.')\n",
        "\n",
        "              #If user ask about egg\n",
        "            elif \"egg\" in message.content.lower() and 'chicken' not in message.content.lower() and 'duck' not in message.content.lower() and 'goose' not in message.content.lower() and 'predict' not in message.content.lower():\n",
        "                await message.channel.send('Currently I can provide information about three types of eggs: chicken egg, duck egg and goose egg. Which one you would like to know?')\n",
        "\n",
        "              #Greeting message\n",
        "            elif message.content.lower().startswith('hello') or message.content.lower().startswith('hi') or message.content.lower().startswith('halo') or message.content.lower().startswith('jackson'):\n",
        "                greeting = ['Harlouu.', 'Hello Human!', 'Yo!', 'Yahallo! haha', 'Wasssupppp!']\n",
        "                res = greeting[random.randint(0, 4)]\n",
        "                await message.channel.send(res.format(message))\n",
        "\n",
        "              #Who are you message\n",
        "            elif message.content.lower().startswith('who are you') or message.content.lower().startswith('who you'):\n",
        "                messageWho = ['I am Jackson!', 'Jackson is my name, an Egg Incubator Bot!', ' I am an Egg Incubator Bot!','Hello! I am Jackson!']\n",
        "                res = messageWho[random.randint(0,3)]\n",
        "                await message.channel.send(res.format(message))\n",
        "\n",
        "              #How are you message\n",
        "            elif message.content.lower().startswith('how are you') or  message.content.lower().startswith('are you okay'):\n",
        "                messageHow = ['I am doing fine.', 'I am hanging in.', 'I am good.', 'I am A-okay! haha', 'Things are a bit tough here, but I will get through.']\n",
        "                res = messageHow[random.randint(0, 4)]\n",
        "                await message.channel.send(res.format(message))\n",
        "\n",
        "              #Goodbye message\n",
        "            elif message.content.lower().startswith('bye') or message.content.lower().startswith('goodbye'):\n",
        "                messageBye = ['Byee, {0.author.mention}!', 'See you next time, {0.author.mention}!', 'It was a great pleasure talking to you, {0.author.mention}!', 'Adios, {0.author.mention}!']\n",
        "                res = messageBye[random.randint(0, 3)]\n",
        "                await message.channel.send(res.format(message))\n",
        "\n",
        "              #thank you message\n",
        "            elif (message.content.lower().startswith('thanks') or message.content.lower().startswith('thank')\n",
        "                or message.content.lower().startswith('tq') or message.content.lower().startswith('3q') or message.content.lower().startswith('10q') or message.content.lower().startswith('thx')):\n",
        "                messageBye = ['You are welcome, {0.author.mention}!', 'My pleasure, {0.author.mention}!']\n",
        "                res = messageBye[random.randint(0, 1)]\n",
        "                await message.channel.send(res.format(message))\n",
        "\n",
        "              #Response to egg types\n",
        "              #Chicken Egg\n",
        "            elif \"chicken\" in message.content.lower() or \"chicken egg\" in message.content.lower() and 'predict' not in message.content.lower() and 'calculate' not in message.content.lower():\n",
        "                egg_info = egg_types['chicken']\n",
        "                await message.channel.send('To hatch chicken eggs:')\n",
        "                await message.channel.send(f'- Incubate the eggs for {egg_info[\"incubation_time\"]} days.')\n",
        "                await message.channel.send(f'- Maintain a temperature of {egg_info[\"ideal_temperature\"]} degrees Celsius.')\n",
        "                await message.channel.send(f'- Maintain a humidity level between {egg_info[\"ideal_humidity\"][0]}% and {egg_info[\"ideal_humidity\"][1]}%.')\n",
        "                await message.channel.send(f'- Turn the eggs {egg_info[\"turning_of_eggs\"]} times per day.')\n",
        "\n",
        "              #Duck Egg\n",
        "            elif \"duck\" in message.content.lower() or \"duck egg\" in message.content.lower() and 'predict' not in message.content.lower() and 'calculate' not in message.content.lower():\n",
        "                egg_info = egg_types['duck']\n",
        "                await message.channel.send('To hatch duck eggs:')\n",
        "                await message.channel.send(f'- Incubate the eggs for {egg_info[\"incubation_time\"]} days.')\n",
        "                await message.channel.send(f'- Maintain a temperature of {egg_info[\"ideal_temperature\"]} degrees Celsius.')\n",
        "                await message.channel.send(f'- Maintain a humidity level between {egg_info[\"ideal_humidity\"][0]}% and {egg_info[\"ideal_humidity\"][1]}%.')\n",
        "                await message.channel.send(f'- Turn the eggs {egg_info[\"turning_of_eggs\"]} times per day.')\n",
        "\n",
        "              #Goose Egg\n",
        "            elif \"goose\" in message.content.lower() or \"goose egg\" in message.content.lower() and 'predict' not in message.content.lower() and 'calculate' not in message.content.lower():\n",
        "                egg_info = egg_types['goose']\n",
        "                await message.channel.send('To hatch goose eggs:')\n",
        "                await message.channel.send(f'- Incubate the eggs for {egg_info[\"incubation_time\"]} days.')\n",
        "                await message.channel.send(f'- Maintain a temperature of {egg_info[\"ideal_temperature\"]} degrees Celsius.')\n",
        "                await message.channel.send(f'- Maintain a humidity level between {egg_info[\"ideal_humidity\"][0]}% and {egg_info[\"ideal_humidity\"][1]}%.')\n",
        "                await message.channel.send(f'- Turn the eggs {egg_info[\"turning_of_eggs\"]} times per day.')\n",
        "\n",
        "              #Prediction here\n",
        "            elif \"predict\" in message.content.lower() or \"success rate\" in message.content.lower():\n",
        "                await message.channel.send('Yes I can!')\n",
        "                await message.channel.send('I have 2 types of prediction model: Fuzzy Logic & Tensorflow ')\n",
        "                await message.channel.send('Please specify')\n",
        "\n",
        "            elif \"fuzzy logic\" in message.content.lower():\n",
        "                await message.channel.send('Please provide me the humidity: ')\n",
        "                #Make sure the imput is from the one who wants to predict\n",
        "                def check(m):\n",
        "                    return m.author.id == message.author.id and m.channel == message.channel\n",
        "                humi = await self.wait_for('message', check=check)\n",
        "                inputHumidity = float(humi.content)\n",
        "\n",
        "                await message.channel.send('Please provide me the temperature: ')\n",
        "                #Make sure the imput is from the one who wants to predict\n",
        "                def check(m):\n",
        "                    return m.author.id == message.author.id and m.channel == message.channel\n",
        "                tem = await self.wait_for('message', check=check)\n",
        "                inputTemperature = float(tem.content)\n",
        "\n",
        "                #Function call to prediction and round off to two-decimal points\n",
        "                #To call : generatePredicition(variable/value,variable/value)\n",
        "                prediction = round(generatePrediction(inputHumidity,inputTemperature),2)\n",
        "                if prediction < 50:\n",
        "                  await message.channel.send(\"Incubation FAILED with the rate of {}% \".format(prediction))\n",
        "                else:\n",
        "                  await message.channel.send(\"Incubation SUCCESSFUL with the rate of {}%\".format(prediction))\n",
        "\n",
        "            elif \"tensorflow\" in message.content.lower():\n",
        "              await message.channel.send(\"Please specify if the bulb is ON or OFF\")\n",
        "              def check(m):\n",
        "                return m.author.id == message.author.id and m.channel == message.channel\n",
        "              bulb = await self.wait_for('message',check=check)\n",
        "              inputBulb = bulb.content.lower()\n",
        "              if \"on\" in inputBulb:\n",
        "                  bulbData = 1\n",
        "              else:\n",
        "                  bulbData = 0\n",
        "              await message.channel.send(\"Please specify if the fan is ON or OFF\")\n",
        "              def check(m):\n",
        "                return m.author.id == message.author.id and m.channel == message.channel\n",
        "              fan = await self.wait_for('message', check=check)\n",
        "              inputFan = fan.content.lower()\n",
        "              if \"on\" in inputFan:\n",
        "                  fanData = 1\n",
        "              else:\n",
        "                  fanData = 0\n",
        "              await message.channel.send(\"Please specify the humidity\")\n",
        "              def check(m):\n",
        "                  return m.author.id == message.author.id and m.channel == message.channel\n",
        "              humi = await self.wait_for('message', check=check)\n",
        "              inputHumidity = float(humi.content)\n",
        "\n",
        "              await message.channel.send(\"Please specify the temperature\")\n",
        "              def check(m):\n",
        "                  return m.author.id == message.author.id and m.channel == message.channel\n",
        "              tem = await self.wait_for('message', check=check)\n",
        "              inputTemperature = float(tem.content)\n",
        "\n",
        "              tensorflow = predictTensorflow(bulbData,fanData,inputHumidity,inputTemperature)*100\n",
        "              await message.channel.send(f'Prediction: {tensorflow:.2f}% success rate for incubation')\n",
        "\n",
        "            elif message.content.isdigit() or \"on\" in message.content.lower() or \"off\" in message.content.lower():\n",
        "              return\n",
        "\n",
        "            else:\n",
        "              await message.channel.send(\"Sorry I do not understand\")\n",
        "\n",
        "              #Reaction with emoji\n",
        "            if message.content == 'cool' or message.content == 'great':\n",
        "                await message.add_reaction('\\U0001F60E')\n",
        "            if message.content == 'thank you':\n",
        "                await message.add_reaction('\\U0001F497')\n",
        "\n",
        "              #Test for input from user in console\n",
        "            print(\"Received message: \" + message.content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An Error occurred: {e}\")\n",
        "            await message.channel.send(\"Oops! An error occurred. Please try again later.\")\n",
        "\n",
        "\n",
        "    async def on_message_edit(self, before, after):\n",
        "        await before.channel.send(\n",
        "          f'{before.author} edit a message.\\n'\n",
        "          f'Before: {before.content}\\n'\n",
        "          f'After: {after.content}\\n'\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.members = True\n",
        "intents.message_content = True\n",
        "def main():\n",
        "    client = JacksonBot(intents=intents)\n",
        "    client.run(TOKEN)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "id": "05c5eef1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeLPoctvWepN"
      },
      "outputs": [],
      "source": [],
      "id": "DeLPoctvWepN"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}